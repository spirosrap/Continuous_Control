{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Crawler.app\"`\n",
    "- **Windows** (x86): `\"path/to/Crawler_Windows_x86/Crawler.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Crawler_Windows_x86_64/Crawler.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Crawler_Linux/Crawler.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Crawler_Linux/Crawler.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Crawler.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Crawler.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Crawler_Linux_NoVis/Crawler.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n",
      "The state for the first agent looks like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.25000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.78813934e-07  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093168e-01 -1.42857209e-01 -6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339906e+00 -1.42857209e-01\n",
      " -1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093347e-01 -1.42857209e-01 -6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339953e+00 -1.42857209e-01\n",
      " -1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093168e-01 -1.42857209e-01  6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339906e+00 -1.42857209e-01\n",
      "  1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093347e-01 -1.42857209e-01  6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339953e+00 -1.42857209e-01\n",
      "  1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_rl.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "from deep_rl.network import *\n",
    "from deep_rl.component import *\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "        self.episode_rewards = []\n",
    "        self.rewards = None\n",
    "        self.episodic_return = None\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            if ret is not None:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.sum(total_rewards))\n",
    "        self.episode_rewards = episodic_returns\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "        self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            self.rewards = info['all_rewards']\n",
    "            if(self.rewards is not None):\n",
    "                episode = len(self.rewards)\n",
    "            if ret is not None:\n",
    "                self.episodic_return = ret\n",
    "#                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "#                 self.logger.info('Episode %d, steps %d, episodic_return_train %s' % (episode,self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "class PPOAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()\n",
    "        self.opt = config.optimizer_fn(self.network.parameters())\n",
    "        self.total_steps = 0\n",
    "        self.states = self.task.reset()\n",
    "        self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        storage = Storage(config.rollout_length)\n",
    "        states = self.states\n",
    "        for _ in range(config.rollout_length):\n",
    "            prediction = self.network(states)\n",
    "            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n",
    "            self.record_online_return(info)\n",
    "            rewards = config.reward_normalizer(rewards)\n",
    "            next_states = config.state_normalizer(next_states)\n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(rewards).unsqueeze(-1),\n",
    "                         'm': tensor(1 - terminals).unsqueeze(-1),\n",
    "                         's': tensor(states)})\n",
    "            states = next_states\n",
    "            self.total_steps += config.num_workers\n",
    "\n",
    "        self.states = states\n",
    "        prediction = self.network(states)\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "\n",
    "        advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(config.rollout_length)):\n",
    "            returns = storage.r[i] + config.discount * storage.m[i] * returns\n",
    "            if not config.use_gae:\n",
    "                advantages = returns - storage.v[i].detach()\n",
    "            else:\n",
    "                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "                advantages = advantages * config.gae_tau * config.discount * storage.m[i] + td_error\n",
    "            storage.adv[i] = advantages.detach()\n",
    "            storage.ret[i] = returns.detach()\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        for _ in range(config.optimization_epochs):\n",
    "            sampler = random_sample(np.arange(states.size(0)), config.mini_batch_size)\n",
    "            for batch_indices in sampler:\n",
    "                batch_indices = tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                prediction = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - self.config.ppo_ratio_clip,\n",
    "                                          1.0 + self.config.ppo_ratio_clip) * sampled_advantages\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - config.entropy_weight * prediction['ent'].mean()\n",
    "\n",
    "                value_loss = 0.5 * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n",
    "                self.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 16,last 16 episodes, mean rewards  1.97,  steps 12288, 1753.20 steps/s\n",
      "INFO:root:Episode 32,last 32 episodes, mean rewards  3.50,  steps 24576, 1762.41 steps/s\n",
      "INFO:root:Episode 48,last 48 episodes, mean rewards  3.82,  steps 36864, 1811.46 steps/s\n",
      "INFO:root:Episode 64,last 64 episodes, mean rewards  3.98,  steps 49152, 1742.68 steps/s\n",
      "INFO:root:Episode 80,last 80 episodes, mean rewards  4.02,  steps 61440, 1738.18 steps/s\n",
      "INFO:root:Episode 96,last 96 episodes, mean rewards  4.50,  steps 73728, 1787.64 steps/s\n",
      "INFO:root:Episode 112,last 100 episodes, mean rewards  5.38,  steps 86016, 1753.05 steps/s\n",
      "INFO:root:Episode 128,last 100 episodes, mean rewards  5.98,  steps 98304, 1749.27 steps/s\n",
      "INFO:root:Episode 144,last 100 episodes, mean rewards  6.80,  steps 110592, 1730.53 steps/s\n",
      "INFO:root:Episode 160,last 100 episodes, mean rewards  7.23,  steps 122880, 1746.07 steps/s\n",
      "INFO:root:Episode 176,last 100 episodes, mean rewards  7.96,  steps 135168, 1756.54 steps/s\n",
      "INFO:root:Episode 192,last 100 episodes, mean rewards  8.10,  steps 147456, 1777.40 steps/s\n",
      "INFO:root:Episode 208,last 100 episodes, mean rewards  8.46,  steps 159744, 1742.22 steps/s\n",
      "INFO:root:Episode 224,last 100 episodes, mean rewards  8.58,  steps 172032, 1733.11 steps/s\n",
      "INFO:root:Episode 240,last 100 episodes, mean rewards  8.23,  steps 184320, 1766.13 steps/s\n",
      "INFO:root:Episode 256,last 100 episodes, mean rewards  7.88,  steps 196608, 1735.54 steps/s\n",
      "INFO:root:Episode 272,last 100 episodes, mean rewards  8.45,  steps 208896, 1734.17 steps/s\n",
      "INFO:root:Episode 288,last 100 episodes, mean rewards  8.72,  steps 221184, 1761.78 steps/s\n",
      "INFO:root:Episode 304,last 100 episodes, mean rewards  9.07,  steps 233472, 1761.03 steps/s\n",
      "INFO:root:Episode 320,last 100 episodes, mean rewards  9.29,  steps 245760, 1765.90 steps/s\n",
      "INFO:root:Episode 336,last 100 episodes, mean rewards  9.96,  steps 258048, 1763.39 steps/s\n",
      "INFO:root:Episode 352,last 100 episodes, mean rewards  11.09,  steps 270336, 1745.38 steps/s\n",
      "INFO:root:Episode 368,last 100 episodes, mean rewards  11.76,  steps 282624, 1770.22 steps/s\n",
      "INFO:root:Episode 384,last 100 episodes, mean rewards  11.92,  steps 294912, 1759.64 steps/s\n",
      "INFO:root:Episode 400,last 100 episodes, mean rewards  12.29,  steps 307200, 1744.68 steps/s\n",
      "INFO:root:Episode 416,last 100 episodes, mean rewards  12.77,  steps 319488, 1793.93 steps/s\n",
      "INFO:root:Episode 432,last 100 episodes, mean rewards  12.99,  steps 331776, 1761.26 steps/s\n",
      "INFO:root:Episode 448,last 100 episodes, mean rewards  12.74,  steps 344064, 1746.74 steps/s\n",
      "INFO:root:Episode 464,last 100 episodes, mean rewards  12.50,  steps 356352, 1770.91 steps/s\n",
      "INFO:root:Episode 480,last 100 episodes, mean rewards  12.48,  steps 368640, 1741.31 steps/s\n",
      "INFO:root:Episode 496,last 100 episodes, mean rewards  13.06,  steps 380928, 1753.48 steps/s\n",
      "INFO:root:Episode 512,last 100 episodes, mean rewards  13.00,  steps 393216, 1748.34 steps/s\n",
      "INFO:root:Episode 528,last 100 episodes, mean rewards  12.58,  steps 405504, 1733.82 steps/s\n",
      "INFO:root:Episode 544,last 100 episodes, mean rewards  12.49,  steps 417792, 1744.35 steps/s\n",
      "INFO:root:Episode 560,last 100 episodes, mean rewards  12.65,  steps 430080, 1756.86 steps/s\n",
      "INFO:root:Episode 576,last 100 episodes, mean rewards  12.63,  steps 442368, 1734.55 steps/s\n",
      "INFO:root:Episode 592,last 100 episodes, mean rewards  12.45,  steps 454656, 1765.46 steps/s\n",
      "INFO:root:Episode 608,last 100 episodes, mean rewards  12.80,  steps 466944, 1833.22 steps/s\n",
      "INFO:root:Episode 624,last 100 episodes, mean rewards  13.15,  steps 479232, 1752.88 steps/s\n",
      "INFO:root:Episode 640,last 100 episodes, mean rewards  13.63,  steps 491520, 1728.05 steps/s\n",
      "INFO:root:Episode 656,last 100 episodes, mean rewards  13.80,  steps 503808, 1733.47 steps/s\n",
      "INFO:root:Episode 672,last 100 episodes, mean rewards  14.19,  steps 516096, 1789.85 steps/s\n",
      "INFO:root:Episode 688,last 100 episodes, mean rewards  14.44,  steps 528384, 1747.70 steps/s\n",
      "INFO:root:Episode 704,last 100 episodes, mean rewards  14.17,  steps 540672, 1729.73 steps/s\n",
      "INFO:root:Episode 720,last 100 episodes, mean rewards  14.29,  steps 552960, 1722.81 steps/s\n",
      "INFO:root:Episode 736,last 100 episodes, mean rewards  14.79,  steps 565248, 1733.33 steps/s\n",
      "INFO:root:Episode 752,last 100 episodes, mean rewards  14.99,  steps 577536, 1771.41 steps/s\n",
      "INFO:root:Episode 768,last 100 episodes, mean rewards  14.84,  steps 589824, 1738.64 steps/s\n",
      "INFO:root:Episode 784,last 100 episodes, mean rewards  14.76,  steps 602112, 1727.16 steps/s\n",
      "INFO:root:Episode 800,last 100 episodes, mean rewards  14.67,  steps 614400, 1835.62 steps/s\n",
      "INFO:root:Episode 816,last 100 episodes, mean rewards  15.15,  steps 626688, 1792.09 steps/s\n",
      "INFO:root:Episode 832,last 100 episodes, mean rewards  14.96,  steps 638976, 1752.72 steps/s\n",
      "INFO:root:Episode 848,last 100 episodes, mean rewards  14.94,  steps 651264, 1779.95 steps/s\n",
      "INFO:root:Episode 864,last 100 episodes, mean rewards  15.51,  steps 663552, 1718.52 steps/s\n",
      "INFO:root:Episode 880,last 100 episodes, mean rewards  16.14,  steps 675840, 1764.25 steps/s\n",
      "INFO:root:Episode 896,last 100 episodes, mean rewards  17.08,  steps 688128, 1783.82 steps/s\n",
      "INFO:root:Episode 912,last 100 episodes, mean rewards  17.11,  steps 700416, 1738.88 steps/s\n",
      "INFO:root:Episode 928,last 100 episodes, mean rewards  17.90,  steps 712704, 1725.16 steps/s\n",
      "INFO:root:Episode 944,last 100 episodes, mean rewards  18.17,  steps 724992, 1740.61 steps/s\n",
      "INFO:root:Episode 960,last 100 episodes, mean rewards  18.05,  steps 737280, 1723.90 steps/s\n",
      "INFO:root:Episode 976,last 100 episodes, mean rewards  18.38,  steps 749568, 1770.74 steps/s\n",
      "INFO:root:Episode 992,last 100 episodes, mean rewards  18.01,  steps 761856, 1726.61 steps/s\n",
      "INFO:root:Episode 1008,last 100 episodes, mean rewards  17.92,  steps 774144, 1711.18 steps/s\n",
      "INFO:root:Episode 1024,last 100 episodes, mean rewards  17.53,  steps 786432, 1810.33 steps/s\n",
      "INFO:root:Episode 1040,last 100 episodes, mean rewards  17.25,  steps 798720, 1767.95 steps/s\n",
      "INFO:root:Episode 1056,last 100 episodes, mean rewards  17.57,  steps 811008, 1731.12 steps/s\n",
      "INFO:root:Episode 1072,last 100 episodes, mean rewards  17.10,  steps 823296, 1721.04 steps/s\n",
      "INFO:root:Episode 1088,last 100 episodes, mean rewards  17.44,  steps 835584, 1738.72 steps/s\n",
      "INFO:root:Episode 1104,last 100 episodes, mean rewards  17.54,  steps 847872, 1704.87 steps/s\n",
      "INFO:root:Episode 1120,last 100 episodes, mean rewards  17.91,  steps 860160, 1721.38 steps/s\n",
      "INFO:root:Episode 1136,last 100 episodes, mean rewards  18.11,  steps 872448, 1749.46 steps/s\n",
      "INFO:root:Episode 1152,last 100 episodes, mean rewards  18.32,  steps 884736, 1747.66 steps/s\n",
      "INFO:root:Episode 1168,last 100 episodes, mean rewards  18.59,  steps 897024, 1731.48 steps/s\n",
      "INFO:root:Episode 1184,last 100 episodes, mean rewards  19.20,  steps 909312, 1712.13 steps/s\n",
      "INFO:root:Episode 1200,last 100 episodes, mean rewards  19.48,  steps 921600, 1713.23 steps/s\n",
      "INFO:root:Episode 1216,last 100 episodes, mean rewards  19.75,  steps 933888, 1754.18 steps/s\n",
      "INFO:root:Episode 1232,last 100 episodes, mean rewards  20.25,  steps 946176, 1728.70 steps/s\n",
      "INFO:root:Episode 1248,last 100 episodes, mean rewards  20.63,  steps 958464, 1703.27 steps/s\n",
      "INFO:root:Episode 1264,last 100 episodes, mean rewards  20.20,  steps 970752, 1691.96 steps/s\n",
      "INFO:root:Episode 1280,last 100 episodes, mean rewards  20.15,  steps 983040, 1734.34 steps/s\n",
      "INFO:root:Episode 1296,last 100 episodes, mean rewards  20.27,  steps 995328, 1723.92 steps/s\n",
      "INFO:root:Episode 1312,last 100 episodes, mean rewards  20.93,  steps 1007616, 1711.52 steps/s\n",
      "INFO:root:Episode 1328,last 100 episodes, mean rewards  21.10,  steps 1019904, 1707.76 steps/s\n",
      "INFO:root:Episode 1344,last 100 episodes, mean rewards  21.18,  steps 1032192, 1739.91 steps/s\n",
      "INFO:root:Episode 1360,last 100 episodes, mean rewards  21.93,  steps 1044480, 1718.61 steps/s\n",
      "INFO:root:Episode 1376,last 100 episodes, mean rewards  22.89,  steps 1056768, 1691.46 steps/s\n",
      "INFO:root:Episode 1392,last 100 episodes, mean rewards  23.21,  steps 1069056, 1709.30 steps/s\n",
      "INFO:root:Episode 1408,last 100 episodes, mean rewards  23.45,  steps 1081344, 1699.13 steps/s\n",
      "INFO:root:Episode 1424,last 100 episodes, mean rewards  23.26,  steps 1093632, 1724.94 steps/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 1440,last 100 episodes, mean rewards  23.09,  steps 1105920, 1679.45 steps/s\n",
      "INFO:root:Episode 1456,last 100 episodes, mean rewards  22.84,  steps 1118208, 1758.68 steps/s\n",
      "INFO:root:Episode 1472,last 100 episodes, mean rewards  22.37,  steps 1130496, 1698.98 steps/s\n",
      "INFO:root:Episode 1488,last 100 episodes, mean rewards  21.35,  steps 1142784, 1708.62 steps/s\n",
      "INFO:root:Episode 1504,last 100 episodes, mean rewards  20.83,  steps 1155072, 1677.75 steps/s\n",
      "INFO:root:Episode 1520,last 100 episodes, mean rewards  20.68,  steps 1167360, 1712.50 steps/s\n",
      "INFO:root:Episode 1536,last 100 episodes, mean rewards  20.08,  steps 1179648, 1715.55 steps/s\n",
      "INFO:root:Episode 1552,last 100 episodes, mean rewards  19.81,  steps 1191936, 1736.78 steps/s\n",
      "INFO:root:Episode 1568,last 100 episodes, mean rewards  19.43,  steps 1204224, 1753.56 steps/s\n",
      "INFO:root:Episode 1584,last 100 episodes, mean rewards  19.77,  steps 1216512, 1729.35 steps/s\n",
      "INFO:root:Episode 1600,last 100 episodes, mean rewards  19.75,  steps 1228800, 1680.89 steps/s\n",
      "INFO:root:Episode 1616,last 100 episodes, mean rewards  19.68,  steps 1241088, 1699.48 steps/s\n",
      "INFO:root:Episode 1632,last 100 episodes, mean rewards  20.28,  steps 1253376, 1724.78 steps/s\n",
      "INFO:root:Episode 1648,last 100 episodes, mean rewards  20.98,  steps 1265664, 1725.08 steps/s\n",
      "INFO:root:Episode 1664,last 100 episodes, mean rewards  21.87,  steps 1277952, 1696.36 steps/s\n",
      "INFO:root:Episode 1680,last 100 episodes, mean rewards  22.35,  steps 1290240, 1692.72 steps/s\n",
      "INFO:root:Episode 1696,last 100 episodes, mean rewards  23.06,  steps 1302528, 1692.65 steps/s\n",
      "INFO:root:Episode 1712,last 100 episodes, mean rewards  23.68,  steps 1314816, 1719.91 steps/s\n",
      "INFO:root:Episode 1728,last 100 episodes, mean rewards  23.54,  steps 1327104, 1683.21 steps/s\n",
      "INFO:root:Episode 1744,last 100 episodes, mean rewards  23.62,  steps 1339392, 1713.66 steps/s\n",
      "INFO:root:Episode 1760,last 100 episodes, mean rewards  23.84,  steps 1351680, 1679.25 steps/s\n",
      "INFO:root:Episode 1776,last 100 episodes, mean rewards  23.04,  steps 1363968, 1674.77 steps/s\n",
      "INFO:root:Episode 1792,last 100 episodes, mean rewards  22.53,  steps 1376256, 1683.49 steps/s\n",
      "INFO:root:Episode 1808,last 100 episodes, mean rewards  22.14,  steps 1388544, 1658.50 steps/s\n",
      "INFO:root:Episode 1824,last 100 episodes, mean rewards  21.85,  steps 1400832, 1677.34 steps/s\n",
      "INFO:root:Episode 1840,last 100 episodes, mean rewards  21.92,  steps 1413120, 1665.73 steps/s\n",
      "INFO:root:Episode 1856,last 100 episodes, mean rewards  20.43,  steps 1425408, 1698.29 steps/s\n",
      "INFO:root:Episode 1872,last 100 episodes, mean rewards  19.43,  steps 1437696, 1677.82 steps/s\n",
      "INFO:root:Episode 1888,last 100 episodes, mean rewards  17.77,  steps 1449984, 1696.31 steps/s\n",
      "INFO:root:Episode 1904,last 100 episodes, mean rewards  15.96,  steps 1462272, 1646.44 steps/s\n",
      "INFO:root:Episode 1920,last 100 episodes, mean rewards  14.55,  steps 1474560, 1679.61 steps/s\n",
      "INFO:root:Episode 1936,last 100 episodes, mean rewards  12.83,  steps 1486848, 1666.16 steps/s\n",
      "INFO:root:Episode 1952,last 100 episodes, mean rewards  11.58,  steps 1499136, 1649.22 steps/s\n",
      "INFO:root:Episode 1968,last 100 episodes, mean rewards  10.57,  steps 1511424, 1668.89 steps/s\n",
      "INFO:root:Episode 1984,last 100 episodes, mean rewards  10.68,  steps 1523712, 1653.54 steps/s\n",
      "INFO:root:Episode 2000,last 100 episodes, mean rewards  11.01,  steps 1536000, 1655.13 steps/s\n",
      "INFO:root:Episode 2016,last 100 episodes, mean rewards  10.26,  steps 1548288, 1644.18 steps/s\n",
      "INFO:root:Episode 2032,last 100 episodes, mean rewards  8.92,  steps 1560576, 1664.40 steps/s\n",
      "INFO:root:Episode 2048,last 100 episodes, mean rewards  8.59,  steps 1572864, 1628.03 steps/s\n",
      "INFO:root:Episode 2064,last 100 episodes, mean rewards  8.65,  steps 1585152, 1673.36 steps/s\n",
      "INFO:root:Episode 2080,last 100 episodes, mean rewards  8.47,  steps 1597440, 1692.81 steps/s\n",
      "INFO:root:Episode 2096,last 100 episodes, mean rewards  8.45,  steps 1609728, 1649.42 steps/s\n",
      "INFO:root:Episode 2112,last 100 episodes, mean rewards  9.62,  steps 1622016, 1639.40 steps/s\n",
      "INFO:root:Episode 2128,last 100 episodes, mean rewards  10.78,  steps 1634304, 1659.52 steps/s\n",
      "INFO:root:Episode 2144,last 100 episodes, mean rewards  12.18,  steps 1646592, 1671.03 steps/s\n",
      "INFO:root:Episode 2160,last 100 episodes, mean rewards  13.26,  steps 1658880, 1717.46 steps/s\n",
      "INFO:root:Episode 2176,last 100 episodes, mean rewards  14.23,  steps 1671168, 1685.20 steps/s\n",
      "INFO:root:Episode 2192,last 100 episodes, mean rewards  15.34,  steps 1683456, 1693.14 steps/s\n",
      "INFO:root:Episode 2208,last 100 episodes, mean rewards  15.68,  steps 1695744, 1649.49 steps/s\n",
      "INFO:root:Episode 2224,last 100 episodes, mean rewards  15.54,  steps 1708032, 1702.61 steps/s\n",
      "INFO:root:Episode 2240,last 100 episodes, mean rewards  15.31,  steps 1720320, 1670.02 steps/s\n",
      "INFO:root:Episode 2256,last 100 episodes, mean rewards  14.88,  steps 1732608, 1683.85 steps/s\n",
      "INFO:root:Episode 2272,last 100 episodes, mean rewards  14.74,  steps 1744896, 1670.67 steps/s\n",
      "INFO:root:Episode 2288,last 100 episodes, mean rewards  14.52,  steps 1757184, 1651.32 steps/s\n",
      "INFO:root:Episode 2304,last 100 episodes, mean rewards  14.70,  steps 1769472, 1654.91 steps/s\n",
      "INFO:root:Episode 2320,last 100 episodes, mean rewards  15.13,  steps 1781760, 1654.15 steps/s\n",
      "INFO:root:Episode 2336,last 100 episodes, mean rewards  16.62,  steps 1794048, 1684.84 steps/s\n",
      "INFO:root:Episode 2352,last 100 episodes, mean rewards  16.83,  steps 1806336, 1634.82 steps/s\n",
      "INFO:root:Episode 2368,last 100 episodes, mean rewards  17.02,  steps 1818624, 1658.21 steps/s\n",
      "INFO:root:Episode 2384,last 100 episodes, mean rewards  17.21,  steps 1830912, 1632.79 steps/s\n",
      "INFO:root:Episode 2400,last 100 episodes, mean rewards  17.47,  steps 1843200, 1686.39 steps/s\n",
      "INFO:root:Episode 2416,last 100 episodes, mean rewards  17.37,  steps 1855488, 1653.87 steps/s\n",
      "INFO:root:Episode 2432,last 100 episodes, mean rewards  17.32,  steps 1867776, 1713.81 steps/s\n",
      "INFO:root:Episode 2448,last 100 episodes, mean rewards  17.57,  steps 1880064, 1655.20 steps/s\n",
      "INFO:root:Episode 2464,last 100 episodes, mean rewards  17.98,  steps 1892352, 1665.37 steps/s\n",
      "INFO:root:Episode 2480,last 100 episodes, mean rewards  18.35,  steps 1904640, 1654.11 steps/s\n",
      "INFO:root:Episode 2496,last 100 episodes, mean rewards  18.21,  steps 1916928, 1649.90 steps/s\n",
      "INFO:root:Episode 2512,last 100 episodes, mean rewards  18.63,  steps 1929216, 1642.52 steps/s\n",
      "INFO:root:Episode 2528,last 100 episodes, mean rewards  18.65,  steps 1941504, 1641.55 steps/s\n",
      "INFO:root:Episode 2544,last 100 episodes, mean rewards  17.97,  steps 1953792, 1619.60 steps/s\n",
      "INFO:root:Episode 2560,last 100 episodes, mean rewards  15.40,  steps 1966080, 1637.31 steps/s\n",
      "INFO:root:Episode 2576,last 100 episodes, mean rewards  13.15,  steps 1978368, 1625.54 steps/s\n",
      "INFO:root:Episode 2592,last 100 episodes, mean rewards  11.70,  steps 1990656, 1629.53 steps/s\n",
      "INFO:root:Episode 2608,last 100 episodes, mean rewards  9.75,  steps 2002944, 1657.21 steps/s\n",
      "INFO:root:Episode 2624,last 100 episodes, mean rewards  7.91,  steps 2015232, 1693.70 steps/s\n",
      "INFO:root:Episode 2640,last 100 episodes, mean rewards  6.55,  steps 2027520, 1604.90 steps/s\n",
      "INFO:root:Episode 2656,last 100 episodes, mean rewards  6.44,  steps 2039808, 1614.32 steps/s\n",
      "INFO:root:Episode 2672,last 100 episodes, mean rewards  6.95,  steps 2052096, 1609.60 steps/s\n",
      "INFO:root:Episode 2688,last 100 episodes, mean rewards  6.64,  steps 2064384, 1697.78 steps/s\n",
      "INFO:root:Episode 2704,last 100 episodes, mean rewards  6.31,  steps 2076672, 1625.59 steps/s\n",
      "INFO:root:Episode 2720,last 100 episodes, mean rewards  6.43,  steps 2088960, 1644.37 steps/s\n",
      "INFO:root:Episode 2736,last 100 episodes, mean rewards  5.86,  steps 2101248, 1632.08 steps/s\n",
      "INFO:root:Episode 2752,last 100 episodes, mean rewards  6.88,  steps 2113536, 1618.88 steps/s\n",
      "INFO:root:Episode 2768,last 100 episodes, mean rewards  7.74,  steps 2125824, 1645.18 steps/s\n",
      "INFO:root:Episode 2784,last 100 episodes, mean rewards  8.02,  steps 2138112, 1612.73 steps/s\n",
      "INFO:root:Episode 2800,last 100 episodes, mean rewards  8.82,  steps 2150400, 1629.96 steps/s\n",
      "INFO:root:Episode 2816,last 100 episodes, mean rewards  9.43,  steps 2162688, 1597.04 steps/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 2832,last 100 episodes, mean rewards  10.40,  steps 2174976, 1597.73 steps/s\n",
      "INFO:root:Episode 2848,last 100 episodes, mean rewards  10.80,  steps 2187264, 1613.28 steps/s\n",
      "INFO:root:Episode 2864,last 100 episodes, mean rewards  10.77,  steps 2199552, 1594.53 steps/s\n",
      "INFO:root:Episode 2880,last 100 episodes, mean rewards  10.95,  steps 2211840, 1608.75 steps/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0cce1a07c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_deque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b0cce1a07c28>\u001b[0m in \u001b[0;36mppo_continuous\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_normalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeanStdNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_deque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b0cce1a07c28>\u001b[0m in \u001b[0;36mrun_steps_custom\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8b75ae3fed1a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_steps_custom(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    rewards_all = []\n",
    "    while True:\n",
    "        rewards = agent.episodic_return\n",
    "        if rewards is not None:\n",
    "            rewards_deque.append(np.mean(rewards))\n",
    "            rewards_all.append(np.mean(rewards))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval and (rewards is not None):\n",
    "            agent.logger.info('Episode %d,last %d episodes, mean rewards  %.2f,  steps %d, %.2f steps/s' % (len(rewards_all),len(rewards_deque),np.mean(rewards_deque),agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "#         if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "        if (rewards is not None) and np.mean(rewards_deque) > 2000:\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "            agent.close()\n",
    "            return True,rewards_deque,rewards_all\n",
    "        if (len(rewards_all) % 100):\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "\n",
    "\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "\n",
    "class CrawlerTask():\n",
    "    def __init__(self):\n",
    "#         BaseTask.__init__(self)\n",
    "        self.name = 'Reacher'\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "        self.info = {\"all_rewards\":None}\n",
    "        self.total_rewards = np.zeros(12)\n",
    "        self.rewards = []\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations   # next state\n",
    "        reward = env_info.rewards                   # reward\n",
    "        done = env_info.local_done\n",
    "\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        if np.any(done):\n",
    "            if any(np.isnan(self.total_rewards.reshape(-1))):\n",
    "                self.total_rewards[np.isnan(self.total_rewards)] = -5            \n",
    "            self.info['episodic_return'] = self.total_rewards\n",
    "            self.rewards.append(self.total_rewards)\n",
    "            self.info['all_rewards'] = self.rewards\n",
    "            self.total_rewards = np.zeros(12)\n",
    "            next_state = self.reset()            \n",
    "        else:\n",
    "            self.info['episodic_return'] = None\n",
    "\n",
    "        return np.array(next_state), np.array(reward), np.array(done), self.info\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        return 10\n",
    "\n",
    "def ppo_continuous():\n",
    "    config = Config()\n",
    "    config.num_workers = num_agents\n",
    "    task_fn = lambda : CrawlerTask()\n",
    "    config.task_fn = task_fn\n",
    "    config.eval_env = task_fn()\n",
    "\n",
    "    config.network_fn = lambda: GaussianActorCriticNet(\n",
    "        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim,hidden_units=(128, 128)),\n",
    "        critic_body=FCBody(config.state_dim,hidden_units=(128, 128)))\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, 3e-4, eps=1e-5)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.99\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 64\n",
    "    config.optimization_epochs = 2\n",
    "    config.mini_batch_size = 128\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 4096*2\n",
    "    config.max_steps = 1e8\n",
    "    config.state_normalizer = MeanStdNormalizer()\n",
    "    return run_steps_custom(PPOAgent(config))\n",
    "\n",
    "success, rewards_deque, rewards_all = ppo_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
