{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n",
      "The state for the first agent looks like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.25000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.78813934e-07  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093168e-01 -1.42857209e-01 -6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339906e+00 -1.42857209e-01\n",
      " -1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093347e-01 -1.42857209e-01 -6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339953e+00 -1.42857209e-01\n",
      " -1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093168e-01 -1.42857209e-01  6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339906e+00 -1.42857209e-01\n",
      "  1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093347e-01 -1.42857209e-01  6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339953e+00 -1.42857209e-01\n",
      "  1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Crawler_Linux_NoVis/Crawler.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_rl.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "from deep_rl.network import *\n",
    "from deep_rl.component import *\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "        self.episode_rewards = []\n",
    "        self.rewards = None\n",
    "        self.rewards_deque = None\n",
    "        self.episodic_return = None\n",
    "        self.best_score = 0\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        rewards = np.zeros(12)\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if np.any(done):\n",
    "                ret = rewards\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.mean(total_rewards))\n",
    "        self.episode_rewards = episodic_returns\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "#         self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        if np.mean(episodic_returns) > self.best_score and ((np.std(episodic_returns) / np.sqrt(len(episodic_returns))) > 0.01):\n",
    "            self.save('./data/model-DDPG.bin')\n",
    "            self.best_score = np.mean(episodic_returns)   \n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            if ret is not None:\n",
    "                ret = np.mean(info['episodic_return'])\n",
    "            self.rewards = info['all_rewards']\n",
    "            self.rewards_deque = info['rewards_deque']\n",
    "            if(self.rewards is not None):\n",
    "                episode = len(self.rewards)\n",
    "            if ret is not None and (episode % 100 == 0):\n",
    "                self.episodic_return = ret\n",
    "#                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "#                 self.logger.info('Episode %d, steps %d, episodic_return_train %s' % (episode,self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "#     def record_online_return(self, info, offset=0):\n",
    "#         if isinstance(info, dict):\n",
    "#             ret = info['episodic_return']\n",
    "#             if ret is not None:\n",
    "#                 ret = np.mean(ret)\n",
    "# #                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "# #                 self.logger.info('steps %d, episodic_return_train %s' % (self.total_steps + offset, ret))\n",
    "#         elif isinstance(info, tuple):\n",
    "#             for i, info_ in enumerate(info):\n",
    "#                 self.record_online_return(info_, i)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "            \n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "class DDPGAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.total_steps = 0\n",
    "        self.state = None\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.detach_()\n",
    "            target_param.copy_(target_param * (1.0 - self.config.target_network_mix) +\n",
    "                               param * self.config.target_network_mix)\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        self.config.state_normalizer.set_read_only()\n",
    "        state = self.config.state_normalizer(state)\n",
    "        action = self.network(state)\n",
    "        self.config.state_normalizer.unset_read_only()\n",
    "        return to_np(action)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        if self.state is None:\n",
    "            self.random_process.reset_states()\n",
    "            self.state = self.task.reset()\n",
    "            self.state = config.state_normalizer(self.state)\n",
    "\n",
    "        if self.total_steps < 0: # if self.total_steps < config.warm_up:\n",
    "            action = [self.task.action_space.sample()]\n",
    "        else:\n",
    "            action = self.network(self.state)\n",
    "            action = to_np(action)\n",
    "            action += self.random_process.sample()\n",
    "        action = np.clip(action, -1, 1)\n",
    "#         action = np.clip(action, self.task.action_space.low, self.task.action_space.high)        \n",
    "        next_state, reward, done, info = self.task.step(action)\n",
    "        next_state = self.config.state_normalizer(next_state)\n",
    "        self.record_online_return(info)\n",
    "        reward = self.config.reward_normalizer(reward)\n",
    "\n",
    "        experiences = list(zip(self.state, action, reward, next_state, done))\n",
    "        self.replay.feed_batch(experiences)\n",
    "        \n",
    "#         if done[0]:\n",
    "#             self.random_process.reset_states()\n",
    "        if np.any(done):\n",
    "            self.random_process.reset_states()\n",
    "\n",
    "            \n",
    "        self.state = next_state\n",
    "        self.total_steps += 1\n",
    "        \n",
    "        if self.replay.size() >= config.warm_up:\n",
    "            experiences = self.replay.sample()\n",
    "            states, actions, rewards, next_states, terminals = experiences\n",
    "            states = tensor(states)\n",
    "            actions = tensor(actions)\n",
    "            rewards = tensor(rewards).unsqueeze(-1)\n",
    "            next_states = tensor(next_states)\n",
    "            mask = tensor(1 - terminals).unsqueeze(-1)\n",
    "\n",
    "            phi_next = self.target_network.feature(next_states)\n",
    "            a_next = self.target_network.actor(phi_next)\n",
    "            q_next = self.target_network.critic(phi_next, a_next)\n",
    "            q_next = config.discount * mask * q_next\n",
    "            q_next.add_(rewards)\n",
    "            q_next = q_next.detach()\n",
    "            phi = self.network.feature(states)\n",
    "            q = self.network.critic(phi, actions)\n",
    "            critic_loss = (q - q_next).pow(2).mul(0.5).sum(-1).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.network.critic_opt.step()\n",
    "\n",
    "            phi = self.network.feature(states)\n",
    "            action = self.network.actor(phi)\n",
    "            policy_loss = -self.network.critic(phi.detach(), action).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.network.actor_opt.step()\n",
    "\n",
    "            self.soft_update(self.target_network, self.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.36691351 60.94302569 53.68047563 62.80065738 60.94780986 46.41073941\n",
      " 53.64643728 59.26645052 53.64720165 51.92119759 46.40961678 53.64566409]\n",
      "[46.36835261 60.94152983 53.66961987 62.80444224 60.94746152 46.40976042\n",
      " 53.64644056 59.2430989  53.64355861 51.92580052 46.41207995 53.64669492]\n",
      "[46.36914111 60.9391034  53.67124568 62.8001318  60.94200714 46.41203495\n",
      " 53.64643728 59.26406856 53.64119779 51.92351066 46.41301041 53.64262676]\n",
      "[46.36843417 60.94264391 53.66961987 62.80729213 60.94746152 46.4124224\n",
      " 53.64326122 59.24886562 53.64091565 51.92781532 46.41207995 53.64544154]\n",
      "[46.36564573 60.94007856 53.67644963 62.80327294 60.94178956 46.40742585\n",
      " 53.64541854 59.26538642 53.6409972  51.92091964 46.40982031 53.64721798]\n",
      "[46.36810303 60.94402548 53.68094024 62.80424288 60.94311022 46.40686226\n",
      " 53.64634162 59.26107077 53.6385817  51.92292935 46.41295648 53.64810141]\n",
      "[46.3672579  60.9396341  53.66669454 62.80327294 60.9437475  46.40737394\n",
      " 53.6483718  59.26538642 53.64390635 51.92119759 46.40961678 53.64458742]\n",
      "[46.36699848 60.9361774  53.66861197 62.8001318  60.9435752  46.40341266\n",
      " 53.64437196 59.26538642 53.64732398 51.93028942 46.41301041 53.64566409]\n",
      "[46.3715263  60.94041041 53.68069847 62.80525389 60.94551385 46.40984977\n",
      " 53.6420962  59.26538642 53.64880129 51.92907597 46.41078251 53.64963937]\n",
      "[46.36840566 60.94264391 53.66961987 62.80065738 60.945572   46.41073941\n",
      " 53.64197096 59.26448325 53.64407571 51.92046244 46.4182418  53.64804357]\n",
      "[46.36842265 60.942564   53.6685307  62.81765459 60.94293895 46.4111769\n",
      " 53.63969991 59.26538642 53.64274576 51.92292935 46.41100065 53.64661717]\n",
      "[46.36985499 60.94402548 53.66839334 62.80327294 60.94496898 46.4111769\n",
      " 53.64381446 59.26538642 53.64871833 51.92292935 46.41436344 53.64745832]\n",
      "[46.37093424 60.93773091 53.66961987 62.8001318  60.9461479  46.40625732\n",
      " 53.64326122 59.26538642 53.64471612 51.92638296 46.41207995 53.64721798]\n",
      "[46.36864933 60.94264391 53.66895315 62.81819848 60.94551385 46.40598752\n",
      " 53.64338766 59.26538642 53.64720165 51.92704292 46.41207995 53.64503027]\n",
      "[46.36843417 60.93809834 53.67781977 62.80525389 60.9458822  46.40953952\n",
      " 53.64362966 59.26448325 53.64435309 51.92044057 46.41218244 53.63936491]\n",
      "[46.3672579  60.93508144 53.66779102 62.80293068 60.94678549 46.40448505\n",
      " 53.6420962  59.26538642 53.64751375 51.92298474 46.4142767  53.64963937]\n",
      "[46.36843417 60.94264391 53.66826575 62.8001318  60.94053143 46.41034401\n",
      " 53.64326122 59.26186344 53.64400146 51.92292935 46.41207995 53.65098599]\n",
      "[46.36937597 60.94264391 53.66739316 62.80729213 60.94746152 46.4096612\n",
      " 53.64688188 59.26693133 53.64407571 51.92119759 46.41246404 53.64566409]\n",
      "[46.36843417 60.93510262 53.67624429 62.80293068 60.94746152 46.40932038\n",
      " 53.64326122 59.26380004 53.64407571 51.92604704 46.41607639 53.64642049]\n",
      "[46.36695871 60.93956707 53.66961987 62.80729213 60.94670234 46.40778155\n",
      " 53.64918115 59.26536821 53.64407571 51.92292935 46.41207995 53.64717904]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-121fcbc9f866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_deque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-121fcbc9f866>\u001b[0m in \u001b[0;36mddpg_continuous\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m#     agent.load('data1/model-DDPGAgent.bin')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_deque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg_continuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-121fcbc9f866>\u001b[0m in \u001b[0;36mrun_steps_custom\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#             return True,rewards_deque,rewards_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_all\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/model-%s.bin'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c34926c9f163>\u001b[0m in \u001b[0;36meval_episodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mepisodic_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mepisodic_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisodic_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c34926c9f163>\u001b[0m in \u001b[0;36meval_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-121fcbc9f866>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m   \u001b[0;31m# next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                   \u001b[0;31m# reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/spiros/diskoyext/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_steps_custom(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    rewards_all = []\n",
    "    best_reward = 0\n",
    "    while True:\n",
    "        rewards = agent.rewards\n",
    "        rewards_deque = agent.rewards_deque\n",
    "#         if rewards is not None:\n",
    "#             rewards_deque.append(np.mean(rewards))\n",
    "#             rewards_all.append(np.mean(rewards))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval and (rewards is not None) and (rewards_deque is not None):\n",
    "            agent.logger.info('Episode %d,last %d episodes, mean rewards  %.2f,  steps %d, %.2f steps/s' % (len(rewards),len(rewards_deque),np.mean(rewards_deque),agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "#         if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "#         if (rewards is not None):\n",
    "#             agent.save('./data/model-%s.bin' % (agent_name))\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "        if config.eval_interval and not agent.total_steps % config.eval_interval:\n",
    "            agent.eval_episodes()\n",
    "        if (len(rewards_all) % 200):\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "\n",
    "\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "\n",
    "class CrawlerTask():\n",
    "    def __init__(self):\n",
    "#         BaseTask.__init__(self)\n",
    "        self.name = 'Crawler'\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "        self.info = {\"all_rewards\":None}\n",
    "        self.total_rewards = np.zeros(12)\n",
    "        self.rewards = []\n",
    "        self.rewards_deque = deque(maxlen=100)\n",
    "#         self.action_space = .sample()\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations   # next state\n",
    "        reward = env_info.rewards                   # reward\n",
    "        done = env_info.local_done\n",
    "\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        if np.any(done): \n",
    "            if any(np.isnan(self.total_rewards.reshape(-1))):\n",
    "                self.total_rewards[np.isnan(self.total_rewards)] = -5\n",
    "            self.info['episodic_return'] = self.total_rewards\n",
    "            self.rewards_deque.append(np.mean(self.total_rewards))\n",
    "            self.rewards.append(np.mean(self.total_rewards))\n",
    "            self.info['all_rewards'] = self.rewards\n",
    "            self.info['rewards_deque'] = self.rewards_deque\n",
    "            \n",
    "            self.total_rewards = np.zeros(12)\n",
    "            next_state = self.reset()            \n",
    "        else:\n",
    "            self.info['rewards_deque'] = self.rewards_deque            \n",
    "            self.info['episodic_return'] = None\n",
    "\n",
    "        return np.array(next_state), np.array(reward), np.array(done), self.info\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        return 10\n",
    "    \n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.bn = nn.BatchNorm1d(state_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.bn(x)\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "\n",
    "class TwoLayerFCBodyWithAction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(TwoLayerFCBodyWithAction, self).__init__()\n",
    "        hidden_size1, hidden_size2 = hidden_units\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hidden_size1))\n",
    "        self.fc2 = layer_init(nn.Linear(hidden_size1 + action_dim, hidden_size2))\n",
    "        self.gate = gate\n",
    "        self.feature_dim = hidden_size2\n",
    "        self.bn = nn.BatchNorm1d(state_dim)\n",
    "\n",
    "    def forward(self, x, action):\n",
    "#         x = self.bn(x)        \n",
    "        x = self.gate(self.fc1(x))\n",
    "        phi = self.gate(self.fc2(torch.cat([x, action], dim=1)))\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "\n",
    "def ddpg_continuous(**kwargs):\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.task_fn = lambda: CrawlerTask()\n",
    "    config.eval_env = config.task_fn()\n",
    "    config.max_steps = int(1e6)\n",
    "    config.eval_interval = int(1e4)\n",
    "    config.eval_episodes = 100\n",
    "    config.log_interval = 2000\n",
    "    config.network_fn = lambda: DeterministicActorCriticNet(\n",
    "        config.state_dim, config.action_dim,\n",
    "        actor_body=FCBody(config.state_dim, (400, 300), gate=F.leaky_relu),\n",
    "        critic_body=TwoLayerFCBodyWithAction(\n",
    "            config.state_dim, config.action_dim, (400, 300), gate=F.leaky_relu),\n",
    "        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-4),\n",
    "        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n",
    "\n",
    "    config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=64)\n",
    "    config.discount = 0.95\n",
    "    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "        size=(config.action_dim,), std=LinearSchedule(0.2))\n",
    "    config.warm_up = 100\n",
    "    config.target_network_mix = 1e-3\n",
    "    config.state_normalizer = MeanStdNormalizer()    \n",
    "    agent = DDPGAgent(config)\n",
    "    config.eval_interval\n",
    "#     agent.load('data1/model-DDPGAgent.bin')    \n",
    "    return run_steps_custom(agent)\n",
    "\n",
    "success, rewards_deque, rewards_all = ddpg_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
