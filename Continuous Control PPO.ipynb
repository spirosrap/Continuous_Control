{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install baselines\n",
    "# !git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install\n",
    "# !pip uninstall protobuf -y\n",
    "# !pip install -U protobuf\n",
    "# !pip install scikit-image\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reacher Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "num_agents = states.shape[0]\n",
    "allStates = states\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of the implementatipon of the PPO agent (by Shangtong Zhang )\n",
    "\n",
    "#### About the PPO Algorithm\n",
    "\n",
    "PPO or Proximal Policy Optimization algorithm is an Open AI algorithm released in 2017 that gives improved performance and stability against DDPG and TRPO.\n",
    "\n",
    "![img](https://sarcturus00.github.io/Tidy-Reinforcement-learning/Pseudo_code/PPO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_rl.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "from deep_rl.network import *\n",
    "from deep_rl.component import *\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "        self.episode_rewards = []\n",
    "        self.rewards = None\n",
    "        self.episodic_return = None\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            if ret is not None:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.sum(total_rewards))\n",
    "        self.episode_rewards = episodic_returns\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "        self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            self.rewards = info['all_rewards']\n",
    "            if(self.rewards is not None):\n",
    "                episode = len(self.rewards)\n",
    "            if ret is not None:\n",
    "                self.episodic_return = ret\n",
    "#                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "#                 self.logger.info('Episode %d, steps %d, episodic_return_train %s' % (episode,self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "class PPOAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()\n",
    "        self.opt = config.optimizer_fn(self.network.parameters())\n",
    "        self.total_steps = 0\n",
    "        self.states = self.task.reset()\n",
    "        self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        storage = Storage(config.rollout_length)\n",
    "        states = self.states\n",
    "        for _ in range(config.rollout_length):\n",
    "            prediction = self.network(states)\n",
    "            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n",
    "            self.record_online_return(info)\n",
    "            rewards = config.reward_normalizer(rewards)\n",
    "            next_states = config.state_normalizer(next_states)\n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(rewards).unsqueeze(-1),\n",
    "                         'm': tensor(1 - terminals).unsqueeze(-1),\n",
    "                         's': tensor(states)})\n",
    "            states = next_states\n",
    "            self.total_steps += config.num_workers\n",
    "\n",
    "        self.states = states\n",
    "        prediction = self.network(states)\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "\n",
    "        advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(config.rollout_length)):\n",
    "            returns = storage.r[i] + config.discount * storage.m[i] * returns\n",
    "            if not config.use_gae:\n",
    "                advantages = returns - storage.v[i].detach()\n",
    "            else:\n",
    "                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "                advantages = advantages * config.gae_tau * config.discount * storage.m[i] + td_error\n",
    "            storage.adv[i] = advantages.detach()\n",
    "            storage.ret[i] = returns.detach()\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        for _ in range(config.optimization_epochs):\n",
    "            sampler = random_sample(np.arange(states.size(0)), config.mini_batch_size)\n",
    "            for batch_indices in sampler:\n",
    "                batch_indices = tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                prediction = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - self.config.ppo_ratio_clip,\n",
    "                                          1.0 + self.config.ppo_ratio_clip) * sampled_advantages\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - config.entropy_weight * prediction['ent'].mean()\n",
    "\n",
    "                value_loss = 0.5 * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n",
    "                self.opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Task for the environment and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 1,last 1 episodes, mean rewards  0.27,  steps 163840, 105.20 steps/s\n",
      "INFO:root:Episode 2,last 2 episodes, mean rewards  0.27,  steps 327680, 106.99 steps/s\n",
      "INFO:root:Episode 3,last 3 episodes, mean rewards  0.30,  steps 491520, 106.19 steps/s\n",
      "INFO:root:Episode 4,last 4 episodes, mean rewards  0.33,  steps 655360, 104.33 steps/s\n",
      "INFO:root:Episode 5,last 5 episodes, mean rewards  0.41,  steps 819200, 103.83 steps/s\n",
      "INFO:root:Episode 6,last 6 episodes, mean rewards  0.49,  steps 983040, 114.76 steps/s\n",
      "INFO:root:Episode 7,last 7 episodes, mean rewards  0.57,  steps 1146880, 101.62 steps/s\n",
      "INFO:root:Episode 8,last 8 episodes, mean rewards  0.64,  steps 1310720, 101.48 steps/s\n",
      "INFO:root:Episode 9,last 9 episodes, mean rewards  0.74,  steps 1474560, 101.28 steps/s\n",
      "INFO:root:Episode 10,last 10 episodes, mean rewards  0.85,  steps 1638400, 100.69 steps/s\n",
      "INFO:root:Episode 11,last 11 episodes, mean rewards  0.98,  steps 1802240, 100.00 steps/s\n",
      "INFO:root:Episode 12,last 12 episodes, mean rewards  1.16,  steps 1966080, 112.07 steps/s\n",
      "INFO:root:Episode 13,last 13 episodes, mean rewards  1.30,  steps 2129920, 100.49 steps/s\n",
      "INFO:root:Episode 14,last 14 episodes, mean rewards  1.54,  steps 2293760, 100.06 steps/s\n",
      "INFO:root:Episode 15,last 15 episodes, mean rewards  1.80,  steps 2457600, 100.41 steps/s\n",
      "INFO:root:Episode 16,last 16 episodes, mean rewards  2.10,  steps 2621440, 110.42 steps/s\n",
      "INFO:root:Episode 17,last 17 episodes, mean rewards  2.43,  steps 2785280, 100.92 steps/s\n",
      "INFO:root:Episode 18,last 18 episodes, mean rewards  2.77,  steps 2949120, 101.05 steps/s\n",
      "INFO:root:Episode 19,last 19 episodes, mean rewards  3.15,  steps 3112960, 101.25 steps/s\n",
      "INFO:root:Episode 20,last 20 episodes, mean rewards  3.55,  steps 3276800, 101.03 steps/s\n",
      "INFO:root:Episode 21,last 21 episodes, mean rewards  4.09,  steps 3440640, 100.74 steps/s\n",
      "INFO:root:Episode 22,last 22 episodes, mean rewards  4.66,  steps 3604480, 100.43 steps/s\n",
      "INFO:root:Episode 23,last 23 episodes, mean rewards  5.29,  steps 3768320, 105.62 steps/s\n",
      "INFO:root:Episode 24,last 24 episodes, mean rewards  6.02,  steps 3932160, 100.12 steps/s\n",
      "INFO:root:Episode 25,last 25 episodes, mean rewards  6.78,  steps 4096000, 100.28 steps/s\n",
      "INFO:root:Episode 26,last 26 episodes, mean rewards  7.46,  steps 4259840, 111.07 steps/s\n",
      "INFO:root:Episode 27,last 27 episodes, mean rewards  8.30,  steps 4423680, 99.32 steps/s\n",
      "INFO:root:Episode 28,last 28 episodes, mean rewards  9.16,  steps 4587520, 98.96 steps/s\n",
      "INFO:root:Episode 29,last 29 episodes, mean rewards  9.98,  steps 4751360, 97.22 steps/s\n",
      "INFO:root:Episode 30,last 30 episodes, mean rewards  10.86,  steps 4915200, 99.44 steps/s\n",
      "INFO:root:Episode 31,last 31 episodes, mean rewards  11.61,  steps 5079040, 96.44 steps/s\n",
      "INFO:root:Episode 32,last 32 episodes, mean rewards  12.35,  steps 5242880, 106.33 steps/s\n",
      "INFO:root:Episode 33,last 33 episodes, mean rewards  13.06,  steps 5406720, 95.72 steps/s\n",
      "INFO:root:Episode 34,last 34 episodes, mean rewards  13.74,  steps 5570560, 94.55 steps/s\n",
      "INFO:root:Episode 35,last 35 episodes, mean rewards  14.40,  steps 5734400, 95.29 steps/s\n",
      "INFO:root:Episode 36,last 36 episodes, mean rewards  15.03,  steps 5898240, 94.73 steps/s\n",
      "INFO:root:Episode 37,last 37 episodes, mean rewards  15.64,  steps 6062080, 95.01 steps/s\n",
      "INFO:root:Episode 38,last 38 episodes, mean rewards  16.22,  steps 6225920, 96.08 steps/s\n",
      "INFO:root:Episode 39,last 39 episodes, mean rewards  16.77,  steps 6389760, 104.70 steps/s\n",
      "INFO:root:Episode 40,last 40 episodes, mean rewards  17.30,  steps 6553600, 94.95 steps/s\n",
      "INFO:root:Episode 41,last 41 episodes, mean rewards  17.81,  steps 6717440, 95.17 steps/s\n",
      "INFO:root:Episode 42,last 42 episodes, mean rewards  18.25,  steps 6881280, 94.80 steps/s\n",
      "INFO:root:Episode 43,last 43 episodes, mean rewards  18.70,  steps 7045120, 94.84 steps/s\n",
      "INFO:root:Episode 44,last 44 episodes, mean rewards  19.13,  steps 7208960, 106.10 steps/s\n",
      "INFO:root:Episode 45,last 45 episodes, mean rewards  19.53,  steps 7372800, 94.26 steps/s\n",
      "INFO:root:Episode 46,last 46 episodes, mean rewards  19.94,  steps 7536640, 95.13 steps/s\n",
      "INFO:root:Episode 47,last 47 episodes, mean rewards  20.33,  steps 7700480, 95.09 steps/s\n",
      "INFO:root:Episode 48,last 48 episodes, mean rewards  20.69,  steps 7864320, 105.51 steps/s\n",
      "INFO:root:Episode 49,last 49 episodes, mean rewards  21.05,  steps 8028160, 95.03 steps/s\n",
      "INFO:root:Episode 50,last 50 episodes, mean rewards  21.40,  steps 8192000, 95.09 steps/s\n",
      "INFO:root:Episode 51,last 51 episodes, mean rewards  21.73,  steps 8355840, 95.08 steps/s\n",
      "INFO:root:Episode 52,last 52 episodes, mean rewards  22.05,  steps 8519680, 105.84 steps/s\n",
      "INFO:root:Episode 53,last 53 episodes, mean rewards  22.33,  steps 8683520, 95.59 steps/s\n",
      "INFO:root:Episode 54,last 54 episodes, mean rewards  22.61,  steps 8847360, 94.77 steps/s\n",
      "INFO:root:Episode 55,last 55 episodes, mean rewards  22.90,  steps 9011200, 98.00 steps/s\n",
      "INFO:root:Episode 56,last 56 episodes, mean rewards  23.18,  steps 9175040, 94.86 steps/s\n",
      "INFO:root:Episode 57,last 57 episodes, mean rewards  23.45,  steps 9338880, 93.93 steps/s\n",
      "INFO:root:Episode 58,last 58 episodes, mean rewards  23.69,  steps 9502720, 94.56 steps/s\n",
      "INFO:root:Episode 59,last 59 episodes, mean rewards  23.94,  steps 9666560, 94.82 steps/s\n",
      "INFO:root:Episode 60,last 60 episodes, mean rewards  24.18,  steps 9830400, 95.34 steps/s\n",
      "INFO:root:Episode 61,last 61 episodes, mean rewards  24.41,  steps 9994240, 99.81 steps/s\n",
      "INFO:root:Episode 62,last 62 episodes, mean rewards  24.63,  steps 10158080, 94.33 steps/s\n",
      "INFO:root:Episode 63,last 63 episodes, mean rewards  24.84,  steps 10321920, 104.87 steps/s\n",
      "INFO:root:Episode 64,last 64 episodes, mean rewards  25.05,  steps 10485760, 95.33 steps/s\n",
      "INFO:root:Episode 65,last 65 episodes, mean rewards  25.25,  steps 10649600, 95.09 steps/s\n",
      "INFO:root:Episode 66,last 66 episodes, mean rewards  25.46,  steps 10813440, 97.86 steps/s\n",
      "INFO:root:Episode 67,last 67 episodes, mean rewards  25.65,  steps 10977280, 94.09 steps/s\n",
      "INFO:root:Episode 68,last 68 episodes, mean rewards  25.84,  steps 11141120, 94.68 steps/s\n",
      "INFO:root:Episode 69,last 69 episodes, mean rewards  26.03,  steps 11304960, 94.38 steps/s\n",
      "INFO:root:Episode 70,last 70 episodes, mean rewards  26.20,  steps 11468800, 105.07 steps/s\n",
      "INFO:root:Episode 71,last 71 episodes, mean rewards  26.37,  steps 11632640, 94.35 steps/s\n",
      "INFO:root:Episode 72,last 72 episodes, mean rewards  26.53,  steps 11796480, 105.15 steps/s\n",
      "INFO:root:Episode 73,last 73 episodes, mean rewards  26.70,  steps 11960320, 104.72 steps/s\n",
      "INFO:root:Episode 74,last 74 episodes, mean rewards  26.85,  steps 12124160, 104.72 steps/s\n",
      "INFO:root:Episode 75,last 75 episodes, mean rewards  27.01,  steps 12288000, 104.65 steps/s\n",
      "INFO:root:Episode 76,last 76 episodes, mean rewards  27.16,  steps 12451840, 99.19 steps/s\n",
      "INFO:root:Episode 77,last 77 episodes, mean rewards  27.30,  steps 12615680, 93.98 steps/s\n",
      "INFO:root:Episode 78,last 78 episodes, mean rewards  27.45,  steps 12779520, 94.99 steps/s\n",
      "INFO:root:Episode 79,last 79 episodes, mean rewards  27.58,  steps 12943360, 94.39 steps/s\n",
      "INFO:root:Episode 80,last 80 episodes, mean rewards  27.72,  steps 13107200, 94.90 steps/s\n",
      "INFO:root:Episode 81,last 81 episodes, mean rewards  27.85,  steps 13271040, 94.55 steps/s\n",
      "INFO:root:Episode 82,last 82 episodes, mean rewards  27.98,  steps 13434880, 94.29 steps/s\n",
      "INFO:root:Episode 83,last 83 episodes, mean rewards  28.11,  steps 13598720, 105.38 steps/s\n",
      "INFO:root:Episode 84,last 84 episodes, mean rewards  28.23,  steps 13762560, 105.20 steps/s\n",
      "INFO:root:Episode 85,last 85 episodes, mean rewards  28.34,  steps 13926400, 104.87 steps/s\n",
      "INFO:root:Episode 86,last 86 episodes, mean rewards  28.45,  steps 14090240, 94.37 steps/s\n",
      "INFO:root:Episode 87,last 87 episodes, mean rewards  28.57,  steps 14254080, 94.75 steps/s\n",
      "INFO:root:Episode 88,last 88 episodes, mean rewards  28.68,  steps 14417920, 102.14 steps/s\n",
      "INFO:root:Episode 89,last 89 episodes, mean rewards  28.79,  steps 14581760, 104.27 steps/s\n",
      "INFO:root:Episode 90,last 90 episodes, mean rewards  28.89,  steps 14745600, 94.78 steps/s\n",
      "INFO:root:Episode 91,last 91 episodes, mean rewards  29.00,  steps 14909440, 94.53 steps/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 92,last 92 episodes, mean rewards  29.10,  steps 15073280, 105.50 steps/s\n",
      "INFO:root:Episode 93,last 93 episodes, mean rewards  29.20,  steps 15237120, 104.80 steps/s\n",
      "INFO:root:Episode 94,last 94 episodes, mean rewards  29.31,  steps 15400960, 93.96 steps/s\n",
      "INFO:root:Episode 95,last 95 episodes, mean rewards  29.39,  steps 15564800, 94.06 steps/s\n",
      "INFO:root:Episode 96,last 96 episodes, mean rewards  29.49,  steps 15728640, 94.54 steps/s\n",
      "INFO:root:Episode 97,last 97 episodes, mean rewards  29.58,  steps 15892480, 95.01 steps/s\n",
      "INFO:root:Episode 98,last 98 episodes, mean rewards  29.67,  steps 16056320, 94.47 steps/s\n",
      "INFO:root:Episode 99,last 99 episodes, mean rewards  29.75,  steps 16220160, 104.51 steps/s\n",
      "INFO:root:Episode 100,last 100 episodes, mean rewards  29.84,  steps 16384000, 93.45 steps/s\n",
      "INFO:root:Episode 101,last 100 episodes, mean rewards  30.22,  steps 16547840, 93.63 steps/s\n"
     ]
    }
   ],
   "source": [
    "def run_steps_custom(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    rewards_all = []\n",
    "    while True:\n",
    "        rewards = agent.episodic_return\n",
    "        if rewards is not None:\n",
    "            rewards_deque.append(np.mean(rewards))\n",
    "            rewards_all.append(np.mean(rewards))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval and (rewards is not None):\n",
    "            agent.logger.info('Episode %d,last %d episodes, mean rewards  %.2f,  steps %d, %.2f steps/s' % (len(rewards_all),len(rewards_deque),np.mean(rewards_deque),agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "#         if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "        if (rewards is not None) and np.mean(rewards_deque) > 30:\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "            agent.close()\n",
    "            return True,rewards_deque,rewards_all\n",
    "\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "\n",
    "class ReacherTask():\n",
    "    def __init__(self):\n",
    "#         BaseTask.__init__(self)\n",
    "        self.name = 'Reacher'\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "        self.info = {\"all_rewards\":None}\n",
    "        self.total_rewards = np.zeros(20)\n",
    "        self.rewards = []\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations   # next state\n",
    "        reward = env_info.rewards                   # reward\n",
    "        done = env_info.local_done\n",
    "\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        if np.any(done):\n",
    "            self.info['episodic_return'] = self.total_rewards\n",
    "            self.rewards.append(self.total_rewards)\n",
    "            self.info['all_rewards'] = self.rewards\n",
    "            self.total_rewards = np.zeros(20)\n",
    "            next_state = self.reset()\n",
    "        else:\n",
    "            self.info['episodic_return'] = None\n",
    "\n",
    "        return np.array(next_state), np.array(reward), np.array(done), self.info\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        return 10\n",
    "\n",
    "def ppo_continuous():\n",
    "    config = Config()\n",
    "    config.num_workers = num_agents\n",
    "    task_fn = lambda : ReacherTask()\n",
    "    config.task_fn = task_fn\n",
    "    config.eval_env = task_fn()\n",
    "\n",
    "    config.network_fn = lambda: GaussianActorCriticNet(\n",
    "        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim),\n",
    "        critic_body=FCBody(config.state_dim))\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, 3e-4, eps=1e-5)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 2048*4\n",
    "    config.optimization_epochs = 10\n",
    "    config.mini_batch_size = 32\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 2048*4\n",
    "    config.max_steps = 1e8\n",
    "    config.state_normalizer = MeanStdNormalizer()\n",
    "    return run_steps_custom(PPOAgent(config))\n",
    "\n",
    "success, rewards_deque, rewards_all = ppo_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(rewards_all)+1), rewards_all)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
