{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n",
      "The state for the first agent looks like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.25000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.78813934e-07  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093168e-01 -1.42857209e-01 -6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339906e+00 -1.42857209e-01\n",
      " -1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093347e-01 -1.42857209e-01 -6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339953e+00 -1.42857209e-01\n",
      " -1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093168e-01 -1.42857209e-01  6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339906e+00 -1.42857209e-01\n",
      "  1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093347e-01 -1.42857209e-01  6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339953e+00 -1.42857209e-01\n",
      "  1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Crawler_Linux_NoVis/Crawler.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_rl.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "from deep_rl.network import *\n",
    "from deep_rl.component import *\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "        self.episode_rewards = []\n",
    "        self.rewards = None\n",
    "        self.rewards_deque = None\n",
    "        self.episodic_return = None\n",
    "        self.best_score = 0\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        rewards = np.zeros(12)\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if np.any(done):\n",
    "                ret = rewards\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.mean(total_rewards))\n",
    "        self.episode_rewards = episodic_returns\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "#         self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        if np.mean(episodic_returns) > self.best_score and ((np.std(episodic_returns) / np.sqrt(len(episodic_returns))) > 0.01):\n",
    "            self.save('./data/model-DDPG.bin')\n",
    "            self.best_score = np.mean(episodic_returns)   \n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            if ret is not None:\n",
    "                ret = np.mean(info['episodic_return'])\n",
    "            self.rewards = info['all_rewards']\n",
    "            self.rewards_deque = info['rewards_deque']\n",
    "            if(self.rewards is not None):\n",
    "                episode = len(self.rewards)\n",
    "            if ret is not None and (episode % 100 == 0):\n",
    "                self.episodic_return = ret\n",
    "#                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "#                 self.logger.info('Episode %d, steps %d, episodic_return_train %s' % (episode,self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "#     def record_online_return(self, info, offset=0):\n",
    "#         if isinstance(info, dict):\n",
    "#             ret = info['episodic_return']\n",
    "#             if ret is not None:\n",
    "#                 ret = np.mean(ret)\n",
    "# #                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "# #                 self.logger.info('steps %d, episodic_return_train %s' % (self.total_steps + offset, ret))\n",
    "#         elif isinstance(info, tuple):\n",
    "#             for i, info_ in enumerate(info):\n",
    "#                 self.record_online_return(info_, i)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "            \n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "class DDPGAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.total_steps = 0\n",
    "        self.state = None\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.detach_()\n",
    "            target_param.copy_(target_param * (1.0 - self.config.target_network_mix) +\n",
    "                               param * self.config.target_network_mix)\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        self.config.state_normalizer.set_read_only()\n",
    "        state = self.config.state_normalizer(state)\n",
    "        action = self.network(state)\n",
    "        self.config.state_normalizer.unset_read_only()\n",
    "        return to_np(action)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        if self.state is None:\n",
    "            self.random_process.reset_states()\n",
    "            self.state = self.task.reset()\n",
    "            self.state = config.state_normalizer(self.state)\n",
    "\n",
    "        if self.total_steps < 0: # if self.total_steps < config.warm_up:\n",
    "            action = [self.task.action_space.sample()]\n",
    "        else:\n",
    "            action = self.network(self.state)\n",
    "            action = to_np(action)\n",
    "            action += self.random_process.sample()\n",
    "        action = np.clip(action, -1, 1)\n",
    "#         action = np.clip(action, self.task.action_space.low, self.task.action_space.high)        \n",
    "        next_state, reward, done, info = self.task.step(action)\n",
    "        next_state = self.config.state_normalizer(next_state)\n",
    "        self.record_online_return(info)\n",
    "        reward = self.config.reward_normalizer(reward)\n",
    "\n",
    "        experiences = list(zip(self.state, action, reward, next_state, done))\n",
    "        self.replay.feed_batch(experiences)\n",
    "        if done[0]:\n",
    "            self.random_process.reset_states()\n",
    "        self.state = next_state\n",
    "        self.total_steps += 1\n",
    "\n",
    "        if self.replay.size() >= config.warm_up:\n",
    "            experiences = self.replay.sample()\n",
    "            states, actions, rewards, next_states, terminals = experiences\n",
    "            states = tensor(states)\n",
    "            actions = tensor(actions)\n",
    "            rewards = tensor(rewards).unsqueeze(-1)\n",
    "            next_states = tensor(next_states)\n",
    "            mask = tensor(1 - terminals).unsqueeze(-1)\n",
    "\n",
    "            phi_next = self.target_network.feature(next_states)\n",
    "            a_next = self.target_network.actor(phi_next)\n",
    "            q_next = self.target_network.critic(phi_next, a_next)\n",
    "            q_next = config.discount * mask * q_next\n",
    "            q_next.add_(rewards)\n",
    "            q_next = q_next.detach()\n",
    "            phi = self.network.feature(states)\n",
    "            q = self.network.critic(phi, actions)\n",
    "            critic_loss = (q - q_next).pow(2).mul(0.5).sum(-1).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.network.critic_opt.step()\n",
    "\n",
    "            phi = self.network.feature(states)\n",
    "            action = self.network.actor(phi)\n",
    "            policy_loss = -self.network.critic(phi.detach(), action).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.network.actor_opt.step()\n",
    "\n",
    "            self.soft_update(self.target_network, self.network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:steps 0, episodic_return_test 54.15(0.00)\n",
      "INFO:root:Episode 79,last 79 episodes, mean rewards  0.54,  steps 2000, 6.04 steps/s\n",
      "INFO:root:Episode 234,last 100 episodes, mean rewards  -0.49,  steps 4000, 96.55 steps/s\n",
      "INFO:root:Episode 343,last 100 episodes, mean rewards  -0.35,  steps 6000, 102.45 steps/s\n",
      "INFO:root:Episode 529,last 100 episodes, mean rewards  0.25,  steps 8000, 100.55 steps/s\n",
      "INFO:root:Episode 707,last 100 episodes, mean rewards  0.33,  steps 10000, 104.24 steps/s\n",
      "INFO:root:steps 10000, episodic_return_test 0.70(0.00)\n",
      "INFO:root:Episode 800,last 100 episodes, mean rewards  0.77,  steps 12000, 77.10 steps/s\n",
      "INFO:root:Episode 884,last 100 episodes, mean rewards  0.90,  steps 14000, 97.39 steps/s\n",
      "INFO:root:Episode 949,last 100 episodes, mean rewards  1.93,  steps 16000, 94.21 steps/s\n",
      "INFO:root:Episode 1072,last 100 episodes, mean rewards  1.33,  steps 18000, 98.54 steps/s\n",
      "INFO:root:Episode 1210,last 100 episodes, mean rewards  0.78,  steps 20000, 97.84 steps/s\n",
      "INFO:root:steps 20000, episodic_return_test 0.47(0.00)\n",
      "INFO:root:Episode 1348,last 100 episodes, mean rewards  0.67,  steps 22000, 76.86 steps/s\n",
      "INFO:root:Episode 1517,last 100 episodes, mean rewards  0.24,  steps 24000, 87.54 steps/s\n",
      "INFO:root:Episode 1590,last 100 episodes, mean rewards  1.00,  steps 26000, 95.89 steps/s\n",
      "INFO:root:Episode 1644,last 100 episodes, mean rewards  2.03,  steps 28000, 92.80 steps/s\n",
      "INFO:root:Episode 1809,last 100 episodes, mean rewards  1.01,  steps 30000, 93.87 steps/s\n",
      "INFO:root:steps 30000, episodic_return_test 0.63(0.02)\n",
      "INFO:root:Episode 1927,last 100 episodes, mean rewards  1.27,  steps 32000, 82.19 steps/s\n",
      "INFO:root:Episode 2034,last 100 episodes, mean rewards  1.52,  steps 34000, 103.58 steps/s\n",
      "INFO:root:Episode 2147,last 100 episodes, mean rewards  1.63,  steps 36000, 92.40 steps/s\n",
      "INFO:root:Episode 2272,last 100 episodes, mean rewards  1.33,  steps 38000, 91.95 steps/s\n",
      "INFO:root:Episode 2352,last 100 episodes, mean rewards  1.38,  steps 40000, 93.55 steps/s\n",
      "INFO:root:steps 40000, episodic_return_test 1.85(0.00)\n",
      "INFO:root:Episode 2416,last 100 episodes, mean rewards  2.18,  steps 42000, 67.60 steps/s\n",
      "INFO:root:Episode 2494,last 100 episodes, mean rewards  1.92,  steps 44000, 96.69 steps/s\n",
      "INFO:root:Episode 2564,last 100 episodes, mean rewards  1.97,  steps 46000, 100.44 steps/s\n",
      "INFO:root:Episode 2624,last 100 episodes, mean rewards  2.20,  steps 48000, 97.83 steps/s\n",
      "INFO:root:Episode 2698,last 100 episodes, mean rewards  2.16,  steps 50000, 90.43 steps/s\n",
      "INFO:root:steps 50000, episodic_return_test 2.45(0.01)\n",
      "INFO:root:Episode 2767,last 100 episodes, mean rewards  2.26,  steps 52000, 70.14 steps/s\n",
      "INFO:root:Episode 2840,last 100 episodes, mean rewards  2.37,  steps 54000, 100.19 steps/s\n",
      "INFO:root:Episode 2916,last 100 episodes, mean rewards  2.19,  steps 56000, 96.42 steps/s\n",
      "INFO:root:Episode 3008,last 100 episodes, mean rewards  1.11,  steps 58000, 101.52 steps/s\n",
      "INFO:root:Episode 3107,last 100 episodes, mean rewards  1.59,  steps 60000, 94.49 steps/s\n",
      "INFO:root:steps 60000, episodic_return_test 1.19(0.03)\n",
      "INFO:root:Episode 3220,last 100 episodes, mean rewards  1.43,  steps 62000, 68.00 steps/s\n",
      "INFO:root:Episode 3333,last 100 episodes, mean rewards  0.90,  steps 64000, 91.80 steps/s\n",
      "INFO:root:Episode 3441,last 100 episodes, mean rewards  1.00,  steps 66000, 98.14 steps/s\n",
      "INFO:root:Episode 3524,last 100 episodes, mean rewards  1.10,  steps 68000, 97.11 steps/s\n",
      "INFO:root:Episode 3601,last 100 episodes, mean rewards  2.11,  steps 70000, 95.66 steps/s\n",
      "INFO:root:steps 70000, episodic_return_test 1.78(0.00)\n",
      "INFO:root:Episode 3676,last 100 episodes, mean rewards  2.19,  steps 72000, 74.44 steps/s\n",
      "INFO:root:Episode 3757,last 100 episodes, mean rewards  2.21,  steps 74000, 96.07 steps/s\n",
      "INFO:root:Episode 3836,last 100 episodes, mean rewards  2.87,  steps 76000, 92.46 steps/s\n",
      "INFO:root:Episode 3937,last 100 episodes, mean rewards  3.18,  steps 78000, 91.80 steps/s\n",
      "INFO:root:Episode 4040,last 100 episodes, mean rewards  3.10,  steps 80000, 90.09 steps/s\n",
      "INFO:root:steps 80000, episodic_return_test 1.60(0.04)\n",
      "INFO:root:Episode 4126,last 100 episodes, mean rewards  2.74,  steps 82000, 70.26 steps/s\n",
      "INFO:root:Episode 4218,last 100 episodes, mean rewards  2.34,  steps 84000, 95.42 steps/s\n",
      "INFO:root:Episode 4307,last 100 episodes, mean rewards  5.21,  steps 86000, 94.54 steps/s\n",
      "INFO:root:Episode 4406,last 100 episodes, mean rewards  4.84,  steps 88000, 90.49 steps/s\n",
      "INFO:root:Episode 4482,last 100 episodes, mean rewards  9.02,  steps 90000, 102.13 steps/s\n",
      "INFO:root:steps 90000, episodic_return_test 10.59(0.03)\n",
      "INFO:root:Episode 4563,last 100 episodes, mean rewards  9.00,  steps 92000, 66.56 steps/s\n",
      "INFO:root:Episode 4645,last 100 episodes, mean rewards  9.00,  steps 94000, 88.55 steps/s\n",
      "INFO:root:Episode 4726,last 100 episodes, mean rewards  7.38,  steps 96000, 91.18 steps/s\n",
      "INFO:root:Episode 4804,last 100 episodes, mean rewards  8.72,  steps 98000, 97.28 steps/s\n",
      "INFO:root:Episode 4879,last 100 episodes, mean rewards  8.83,  steps 100000, 90.86 steps/s\n",
      "INFO:root:steps 100000, episodic_return_test 11.47(0.04)\n",
      "INFO:root:Episode 4954,last 100 episodes, mean rewards  11.14,  steps 102000, 61.94 steps/s\n",
      "INFO:root:Episode 5029,last 100 episodes, mean rewards  11.77,  steps 104000, 97.04 steps/s\n",
      "INFO:root:Episode 5106,last 100 episodes, mean rewards  11.00,  steps 106000, 92.86 steps/s\n",
      "INFO:root:Episode 5171,last 100 episodes, mean rewards  11.70,  steps 108000, 90.35 steps/s\n",
      "INFO:root:Episode 5242,last 100 episodes, mean rewards  11.39,  steps 110000, 100.66 steps/s\n",
      "INFO:root:steps 110000, episodic_return_test 0.64(0.00)\n",
      "INFO:root:Episode 5311,last 100 episodes, mean rewards  13.03,  steps 112000, 83.85 steps/s\n",
      "INFO:root:Episode 5379,last 100 episodes, mean rewards  15.06,  steps 114000, 98.50 steps/s\n",
      "INFO:root:Episode 5447,last 100 episodes, mean rewards  13.66,  steps 116000, 97.47 steps/s\n",
      "INFO:root:Episode 5514,last 100 episodes, mean rewards  13.57,  steps 118000, 87.15 steps/s\n",
      "INFO:root:Episode 5586,last 100 episodes, mean rewards  13.50,  steps 120000, 102.10 steps/s\n",
      "INFO:root:steps 120000, episodic_return_test 14.98(0.03)\n",
      "INFO:root:Episode 5664,last 100 episodes, mean rewards  12.88,  steps 122000, 60.19 steps/s\n",
      "INFO:root:Episode 5748,last 100 episodes, mean rewards  10.55,  steps 124000, 88.41 steps/s\n",
      "INFO:root:Episode 5824,last 100 episodes, mean rewards  12.44,  steps 126000, 94.30 steps/s\n",
      "INFO:root:Episode 5899,last 100 episodes, mean rewards  12.01,  steps 128000, 92.80 steps/s\n",
      "INFO:root:Episode 5980,last 100 episodes, mean rewards  12.47,  steps 130000, 91.07 steps/s\n",
      "INFO:root:steps 130000, episodic_return_test 19.70(0.20)\n",
      "INFO:root:Episode 6050,last 100 episodes, mean rewards  14.18,  steps 132000, 64.08 steps/s\n",
      "INFO:root:Episode 6122,last 100 episodes, mean rewards  15.54,  steps 134000, 92.85 steps/s\n",
      "INFO:root:Episode 6203,last 100 episodes, mean rewards  13.64,  steps 136000, 95.23 steps/s\n",
      "INFO:root:Episode 6283,last 100 episodes, mean rewards  14.35,  steps 138000, 93.43 steps/s\n",
      "INFO:root:Episode 6361,last 100 episodes, mean rewards  13.52,  steps 140000, 95.06 steps/s\n",
      "INFO:root:steps 140000, episodic_return_test 17.89(0.20)\n",
      "INFO:root:Episode 6435,last 100 episodes, mean rewards  15.03,  steps 142000, 64.43 steps/s\n",
      "INFO:root:Episode 6510,last 100 episodes, mean rewards  14.54,  steps 144000, 96.48 steps/s\n",
      "INFO:root:Episode 6579,last 100 episodes, mean rewards  14.94,  steps 146000, 95.61 steps/s\n",
      "INFO:root:Episode 6660,last 100 episodes, mean rewards  13.24,  steps 148000, 93.11 steps/s\n",
      "INFO:root:Episode 6740,last 100 episodes, mean rewards  13.99,  steps 150000, 90.06 steps/s\n",
      "INFO:root:steps 150000, episodic_return_test 14.57(0.04)\n",
      "INFO:root:Episode 6819,last 100 episodes, mean rewards  13.92,  steps 152000, 65.17 steps/s\n",
      "INFO:root:Episode 6895,last 100 episodes, mean rewards  15.47,  steps 154000, 89.35 steps/s\n",
      "INFO:root:Episode 6974,last 100 episodes, mean rewards  14.61,  steps 156000, 93.69 steps/s\n",
      "INFO:root:Episode 7044,last 100 episodes, mean rewards  14.42,  steps 158000, 97.26 steps/s\n",
      "INFO:root:Episode 7118,last 100 episodes, mean rewards  15.10,  steps 160000, 91.95 steps/s\n",
      "INFO:root:steps 160000, episodic_return_test 21.77(0.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 7186,last 100 episodes, mean rewards  16.11,  steps 162000, 60.92 steps/s\n",
      "INFO:root:Episode 7255,last 100 episodes, mean rewards  16.69,  steps 164000, 95.13 steps/s\n",
      "INFO:root:Episode 7324,last 100 episodes, mean rewards  17.51,  steps 166000, 97.15 steps/s\n",
      "INFO:root:Episode 7403,last 100 episodes, mean rewards  14.32,  steps 168000, 90.28 steps/s\n",
      "INFO:root:Episode 7475,last 100 episodes, mean rewards  15.55,  steps 170000, 95.77 steps/s\n",
      "INFO:root:steps 170000, episodic_return_test 16.37(0.03)\n",
      "INFO:root:Episode 7554,last 100 episodes, mean rewards  14.76,  steps 172000, 64.57 steps/s\n",
      "INFO:root:Episode 7632,last 100 episodes, mean rewards  13.26,  steps 174000, 94.87 steps/s\n",
      "INFO:root:Episode 7700,last 100 episodes, mean rewards  16.87,  steps 176000, 89.94 steps/s\n",
      "INFO:root:Episode 7768,last 100 episodes, mean rewards  17.12,  steps 178000, 95.37 steps/s\n",
      "INFO:root:Episode 7837,last 100 episodes, mean rewards  15.26,  steps 180000, 90.37 steps/s\n",
      "INFO:root:steps 180000, episodic_return_test 17.87(0.15)\n",
      "INFO:root:Episode 7905,last 100 episodes, mean rewards  16.47,  steps 182000, 60.06 steps/s\n",
      "INFO:root:Episode 7993,last 100 episodes, mean rewards  13.42,  steps 184000, 94.11 steps/s\n",
      "INFO:root:Episode 8066,last 100 episodes, mean rewards  15.60,  steps 186000, 90.40 steps/s\n",
      "INFO:root:Episode 8137,last 100 episodes, mean rewards  16.37,  steps 188000, 88.96 steps/s\n",
      "INFO:root:Episode 8216,last 100 episodes, mean rewards  14.82,  steps 190000, 87.23 steps/s\n",
      "INFO:root:steps 190000, episodic_return_test 21.38(0.14)\n",
      "INFO:root:Episode 8288,last 100 episodes, mean rewards  15.90,  steps 192000, 62.01 steps/s\n",
      "INFO:root:Episode 8352,last 100 episodes, mean rewards  18.40,  steps 194000, 100.63 steps/s\n",
      "INFO:root:Episode 8421,last 100 episodes, mean rewards  17.43,  steps 196000, 95.14 steps/s\n",
      "INFO:root:Episode 8489,last 100 episodes, mean rewards  15.45,  steps 198000, 100.06 steps/s\n",
      "INFO:root:Episode 8542,last 100 episodes, mean rewards  16.44,  steps 200000, 96.77 steps/s\n",
      "INFO:root:steps 200000, episodic_return_test 26.88(0.04)\n",
      "INFO:root:Episode 8609,last 100 episodes, mean rewards  16.90,  steps 202000, 54.65 steps/s\n",
      "INFO:root:Episode 8672,last 100 episodes, mean rewards  17.11,  steps 204000, 88.82 steps/s\n",
      "INFO:root:Episode 8734,last 100 episodes, mean rewards  19.20,  steps 206000, 92.86 steps/s\n",
      "INFO:root:Episode 8795,last 100 episodes, mean rewards  19.71,  steps 208000, 92.86 steps/s\n",
      "INFO:root:Episode 8859,last 100 episodes, mean rewards  19.20,  steps 210000, 95.68 steps/s\n",
      "INFO:root:steps 210000, episodic_return_test 21.76(0.08)\n",
      "INFO:root:Episode 8918,last 100 episodes, mean rewards  19.38,  steps 212000, 56.30 steps/s\n",
      "INFO:root:Episode 8977,last 100 episodes, mean rewards  19.99,  steps 214000, 95.93 steps/s\n",
      "INFO:root:Episode 9036,last 100 episodes, mean rewards  16.79,  steps 216000, 98.63 steps/s\n",
      "INFO:root:Episode 9096,last 100 episodes, mean rewards  17.97,  steps 218000, 99.95 steps/s\n",
      "INFO:root:Episode 9160,last 100 episodes, mean rewards  17.88,  steps 220000, 90.84 steps/s\n",
      "INFO:root:steps 220000, episodic_return_test 23.12(0.18)\n",
      "INFO:root:Episode 9222,last 100 episodes, mean rewards  18.37,  steps 222000, 56.03 steps/s\n",
      "INFO:root:Episode 9280,last 100 episodes, mean rewards  21.70,  steps 224000, 92.36 steps/s\n",
      "INFO:root:Episode 9343,last 100 episodes, mean rewards  20.77,  steps 226000, 86.53 steps/s\n",
      "INFO:root:Episode 9401,last 100 episodes, mean rewards  19.16,  steps 228000, 95.10 steps/s\n",
      "INFO:root:Episode 9462,last 100 episodes, mean rewards  17.66,  steps 230000, 94.06 steps/s\n",
      "INFO:root:steps 230000, episodic_return_test 21.42(0.10)\n",
      "INFO:root:Episode 9523,last 100 episodes, mean rewards  17.48,  steps 232000, 63.93 steps/s\n",
      "INFO:root:Episode 9584,last 100 episodes, mean rewards  20.18,  steps 234000, 92.14 steps/s\n"
     ]
    }
   ],
   "source": [
    "def run_steps_custom(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    rewards_all = []\n",
    "    best_reward = 0\n",
    "    while True:\n",
    "        rewards = agent.rewards\n",
    "        rewards_deque = agent.rewards_deque\n",
    "#         if rewards is not None:\n",
    "#             rewards_deque.append(np.mean(rewards))\n",
    "#             rewards_all.append(np.mean(rewards))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval and (rewards is not None) and (rewards_deque is not None):\n",
    "            agent.logger.info('Episode %d,last %d episodes, mean rewards  %.2f,  steps %d, %.2f steps/s' % (len(rewards),len(rewards_deque),np.mean(rewards_deque),agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "#         if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "#         if (rewards is not None):\n",
    "#             agent.save('./data/model-%s.bin' % (agent_name))\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "        if config.eval_interval and not agent.total_steps % config.eval_interval:\n",
    "            agent.eval_episodes()\n",
    "        if (len(rewards_all) % 200):\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "\n",
    "\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "\n",
    "class CrawlerTask():\n",
    "    def __init__(self):\n",
    "#         BaseTask.__init__(self)\n",
    "        self.name = 'Crawler'\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "        self.info = {\"all_rewards\":None}\n",
    "        self.total_rewards = np.zeros(12)\n",
    "        self.rewards = []\n",
    "        self.rewards_deque = deque(maxlen=100)\n",
    "#         self.action_space = .sample()\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations   # next state\n",
    "        reward = env_info.rewards                   # reward\n",
    "        done = env_info.local_done\n",
    "\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        if np.any(done): \n",
    "            if any(np.isnan(self.total_rewards.reshape(-1))):\n",
    "                self.total_rewards[np.isnan(self.total_rewards)] = -5            \n",
    "            self.info['episodic_return'] = self.total_rewards\n",
    "            self.rewards_deque.append(np.mean(self.total_rewards))\n",
    "            self.rewards.append(self.total_rewards)\n",
    "            self.info['all_rewards'] = self.rewards\n",
    "            self.info['rewards_deque'] = self.rewards_deque\n",
    "            self.total_rewards = np.zeros(12)\n",
    "            next_state = self.reset()            \n",
    "        else:\n",
    "            self.info['rewards_deque'] = self.rewards_deque            \n",
    "            self.info['episodic_return'] = None\n",
    "\n",
    "        return np.array(next_state), np.array(reward), np.array(done), self.info\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        return 10\n",
    "    \n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.bn = nn.BatchNorm1d(state_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.bn(x)\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "\n",
    "class TwoLayerFCBodyWithAction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(TwoLayerFCBodyWithAction, self).__init__()\n",
    "        hidden_size1, hidden_size2 = hidden_units\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hidden_size1))\n",
    "        self.fc2 = layer_init(nn.Linear(hidden_size1 + action_dim, hidden_size2))\n",
    "        self.gate = gate\n",
    "        self.feature_dim = hidden_size2\n",
    "        self.bn = nn.BatchNorm1d(state_dim)\n",
    "\n",
    "    def forward(self, x, action):\n",
    "#         x = self.bn(x)        \n",
    "        x = self.gate(self.fc1(x))\n",
    "        phi = self.gate(self.fc2(torch.cat([x, action], dim=1)))\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "\n",
    "def ddpg_continuous(**kwargs):\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.task_fn = lambda: CrawlerTask()\n",
    "    config.eval_env = config.task_fn()\n",
    "    config.max_steps = int(1e6)\n",
    "    config.eval_interval = int(1e4)\n",
    "    config.eval_episodes = 100\n",
    "    config.log_interval = 2000\n",
    "    config.network_fn = lambda: DeterministicActorCriticNet(\n",
    "        config.state_dim, config.action_dim,\n",
    "        actor_body=FCBody(config.state_dim, (400, 300), gate=F.leaky_relu),\n",
    "        critic_body=TwoLayerFCBodyWithAction(\n",
    "            config.state_dim, config.action_dim, (400, 300), gate=F.leaky_relu),\n",
    "        actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-4),\n",
    "        critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n",
    "\n",
    "    config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=64)\n",
    "    config.discount = 0.99\n",
    "    config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "        size=(config.action_dim,), std=LinearSchedule(0.2))\n",
    "    config.warm_up = 1e4\n",
    "    config.target_network_mix = 1e-3\n",
    "    agent = DDPGAgent(config)\n",
    "#     agent.load('data/model-DDPG.bin')    \n",
    "    return run_steps_custom(agent)\n",
    "\n",
    "success, rewards_deque, rewards_all = ddpg_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
