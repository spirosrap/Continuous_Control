{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install baselines\n",
    "# !git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install\n",
    "# !pip uninstall protobuf -y\n",
    "# !pip install -U protobuf\n",
    "# !pip install scikit-image\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reacher Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n",
      "The state for the first agent looks like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.25000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.78813934e-07  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093168e-01 -1.42857209e-01 -6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339906e+00 -1.42857209e-01\n",
      " -1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093347e-01 -1.42857209e-01 -6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339953e+00 -1.42857209e-01\n",
      " -1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093168e-01 -1.42857209e-01  6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339906e+00 -1.42857209e-01\n",
      "  1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093347e-01 -1.42857209e-01  6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339953e+00 -1.42857209e-01\n",
      "  1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Crawler_Linux_NoVis/Crawler.x86_64')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "num_agents = states.shape[0]\n",
    "allStates = states\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of the implementatipon of the PPO agent (by Shangtong Zhang )\n",
    "\n",
    "#### About the PPO Algorithm\n",
    "\n",
    "PPO or Proximal Policy Optimization algorithm is an Open AI algorithm released in 2017 that gives improved performance and stability against DDPG and TRPO.\n",
    "\n",
    "![img](https://sarcturus00.github.io/Tidy-Reinforcement-learning/Pseudo_code/PPO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.34519732572759193\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2017 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deep_rl.utils import *\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "from deep_rl.network import *\n",
    "from deep_rl.component import *\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "        self.episode_rewards = []\n",
    "        self.rewards = None\n",
    "        self.episodic_return = None\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            if ret is not None:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.sum(total_rewards))\n",
    "        self.episode_rewards = episodic_returns\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "        self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            self.rewards = info['all_rewards']\n",
    "            if(self.rewards is not None):\n",
    "                episode = len(self.rewards)\n",
    "            if ret is not None:\n",
    "                self.episodic_return = ret\n",
    "#                 self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "#                 self.logger.info('Episode %d, steps %d, episodic_return_train %s' % (episode,self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "class PPOAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()\n",
    "        self.opt = config.optimizer_fn(self.network.parameters())\n",
    "        self.total_steps = 0\n",
    "        self.states = self.task.reset()\n",
    "        self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        storage = Storage(config.rollout_length)\n",
    "        states = self.states\n",
    "        for _ in range(config.rollout_length):\n",
    "            prediction = self.network(states)\n",
    "            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n",
    "            self.record_online_return(info)\n",
    "            rewards = config.reward_normalizer(rewards)\n",
    "            next_states = config.state_normalizer(next_states)\n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(rewards).unsqueeze(-1),\n",
    "                         'm': tensor(1 - terminals).unsqueeze(-1),\n",
    "                         's': tensor(states)})\n",
    "            states = next_states\n",
    "            self.total_steps += config.num_workers\n",
    "\n",
    "        self.states = states\n",
    "        prediction = self.network(states)\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "\n",
    "        advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(config.rollout_length)):\n",
    "            returns = storage.r[i] + config.discount * storage.m[i] * returns\n",
    "            if not config.use_gae:\n",
    "                advantages = returns - storage.v[i].detach()\n",
    "            else:\n",
    "                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "                advantages = advantages * config.gae_tau * config.discount * storage.m[i] + td_error\n",
    "            storage.adv[i] = advantages.detach()\n",
    "            storage.ret[i] = returns.detach()\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "        for _ in range(config.optimization_epochs):\n",
    "            sampler = random_sample(np.arange(states.size(0)), config.mini_batch_size)\n",
    "            for batch_indices in sampler:\n",
    "                batch_indices = tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                prediction = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - self.config.ppo_ratio_clip,\n",
    "                                          1.0 + self.config.ppo_ratio_clip) * sampled_advantages\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - config.entropy_weight * prediction['ent'].mean()\n",
    "\n",
    "                value_loss = 0.5 * (sampled_returns - prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n",
    "                self.opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Task for the environment and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode 16,last 16 episodes, mean rewards  2.20,  steps 12288, 63.36 steps/s\n",
      "INFO:root:Episode 32,last 32 episodes, mean rewards  3.35,  steps 24576, 63.53 steps/s\n",
      "INFO:root:Episode 48,last 48 episodes, mean rewards  3.94,  steps 36864, 63.68 steps/s\n",
      "INFO:root:Episode 64,last 64 episodes, mean rewards  3.78,  steps 49152, 64.29 steps/s\n",
      "INFO:root:Episode 80,last 80 episodes, mean rewards  4.03,  steps 61440, 64.30 steps/s\n",
      "INFO:root:Episode 96,last 96 episodes, mean rewards  4.28,  steps 73728, 64.22 steps/s\n",
      "INFO:root:Episode 112,last 100 episodes, mean rewards  4.57,  steps 86016, 64.21 steps/s\n",
      "INFO:root:Episode 128,last 100 episodes, mean rewards  4.88,  steps 98304, 64.77 steps/s\n",
      "INFO:root:Episode 144,last 100 episodes, mean rewards  5.03,  steps 110592, 64.49 steps/s\n",
      "INFO:root:Episode 160,last 100 episodes, mean rewards  5.10,  steps 122880, 65.07 steps/s\n"
     ]
    }
   ],
   "source": [
    "def run_steps_custom(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    rewards_all = []\n",
    "    best_score = 0\n",
    "    while True:\n",
    "        rewards = agent.episodic_return\n",
    "        if rewards is not None:\n",
    "            rewards_deque.append(np.mean(rewards))\n",
    "            rewards_all.append(np.mean(rewards))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval and (rewards is not None):\n",
    "            agent.logger.info('Episode %d,last %d episodes, mean rewards  %.2f,  steps %d, %.2f steps/s' % (len(rewards_all),len(rewards_deque),np.mean(rewards_deque),agent.total_steps, config.log_interval / (time.time() - t0)))\n",
    "            t0 = time.time()\n",
    "            if best_score < np.mean(rewards_deque):\n",
    "                agent.save('./data/model-%s.bin' % (agent_name))\n",
    "                best_score = np.mean(rewards_deque)\n",
    "                \n",
    "#         if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "#             agent.close()\n",
    "#             return True,rewards_deque,rewards_all\n",
    "        if (rewards is not None) and np.mean(rewards_deque) > 500:\n",
    "            agent.save('./data/model-%s.bin' % (agent_name))\n",
    "            agent.close()\n",
    "            return True,rewards_deque,rewards_all\n",
    "\n",
    "        agent.step()\n",
    "        agent.switch_task()\n",
    "\n",
    "class CrawlerTask():\n",
    "    def __init__(self):\n",
    "#         BaseTask.__init__(self)\n",
    "        self.name = 'Crawler'\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "        self.info = {\"all_rewards\":None}\n",
    "        self.total_rewards = np.zeros(num_agents)\n",
    "        self.rewards = []\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations   # next state\n",
    "        reward = env_info.rewards                   # reward\n",
    "        done = env_info.local_done\n",
    "\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        if np.any(done):\n",
    "            if any(np.isnan(self.total_rewards.reshape(-1))):\n",
    "                self.total_rewards[np.isnan(self.total_rewards)] = -5            \n",
    "            self.info['episodic_return'] = self.total_rewards\n",
    "            self.rewards.append(self.total_rewards)\n",
    "            self.info['all_rewards'] = self.rewards\n",
    "            self.total_rewards = np.zeros(num_agents)\n",
    "            next_state = self.reset()\n",
    "        else:\n",
    "            self.info['episodic_return'] = None\n",
    "\n",
    "        return np.array(next_state), np.array(reward), np.array(done), self.info\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        return 10\n",
    "\n",
    "# Increasing the complexity of the actor and critic networks. Changed Hidden Units from 64x64.    \n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(2048, 1024, 512), gate=F.leaky_relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "def ppo_continuous():\n",
    "    config = Config()\n",
    "    config.num_workers = num_agents\n",
    "    task_fn = lambda : CrawlerTask()\n",
    "    config.task_fn = task_fn\n",
    "    config.eval_env = task_fn()\n",
    "\n",
    "    config.network_fn = lambda: GaussianActorCriticNet(\n",
    "        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim),\n",
    "        critic_body=FCBody(config.state_dim))\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, 3e-4, eps=1e-5)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 64\n",
    "    config.optimization_epochs = 10\n",
    "    config.mini_batch_size = 128\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 2048*2\n",
    "    config.max_steps = 1e8\n",
    "    \n",
    "    # Removing Normalizer\n",
    "    # By default it uses the RescaleNormalizer() with coefficient 1 \n",
    "    # which means it's just a dummy normalizer\n",
    "    # See\n",
    "    \n",
    "    #config.state_normalizer = MeanStdNormalizer()\n",
    "    #config.reward_normalizer = MeanStdNormalizer()\n",
    "    \n",
    "    agent = PPOAgent(config)\n",
    "#     agent.load('data/model-PPOAgent.bin')    \n",
    "    return run_steps_custom(agent)\n",
    "\n",
    "success, rewards_deque, rewards_all = ppo_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(rewards_all)+1), rewards_all)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
